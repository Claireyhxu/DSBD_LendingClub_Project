{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Intermediate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('LC_15_18.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns we discarded\n",
    "- 100% missing values: `id`, `member_id`, `url`, `next_pymnt_d`\n",
    "- The same for all rows: `pymnt_plan`, `out_prncp`, `out_prncp_inv`, and `policy_code`\n",
    "- Updated in past 2 months -- wouldn't have these values at loan origination: `num_tl_120dpd_2m`, and `num_tl_30dpd`\n",
    "- Hardship-related variables, relating to borrowers who were on a hardship plan -- wouldn't know at the point of loan origination whether or not the borrower will have a hardship: `deferral_term`, `hardship_amount`, `hardship_dpd`, `hardship_end_date`, `hardship_flag`, `hardship_last_payment_amount`, `hardship_length`, `hardship_loan_status`, `hardship_payoff_balance_amount`, `hardship_reason`, `hardship_start_date`, `hardship_status`, `hardship_type`, `orig_projected_additional_accrued_interest`, `payment_plan_start_date`\n",
    "- String variables provided by borrowers. LendingClub categorized them into other variables: `desc`, `title`, and `emp_title`\n",
    "- Loan settlement/charged off related variables -- wouldn't know them at the time of funding: `debt_settlement_flag`, `debt_settlement_flag_date`, `settlement_amount`, `settlement_date`, `settlement_percentage`, `settlement_status`, `settlement_term`, `recoveries`, `collection_recovery_fee`\n",
    "- Loan payment related variables -- wouldn't know them at the time of funding: `funded_amnt`, `funded_amnt_inv`, `last_pymnt_amnt`, `out_prncp`, `out_prncp_inv`, `total_pymnt`, `total_pymnt_inv`, `total_rec_int`, `total_rec_late_fee`, `total_rec_prncp`, `last_pymnt_d`, `last_credit_pull_d`, `disbursement_method`\n",
    "- `initial_list_status`: as the values \"W' and \"F\" are randomly assigned\n",
    "- Columns relating to co-borrowers. These columns only have non-null values from issue dates of 2017-03-01 and later (or 2015-10-01 and later for 'dti_joint', 'annual_inc_joint', and 'verification_status_joint'), and have around 97% missing values, so we remove these columns: `annual_inc_joint`, `dti_joint`, `revol_bal_joint`, `sec_app_chargeoff_within_12_mths`, `sec_app_collections_12_mths_ex_med`, `sec_app_earliest_cr_line`, `sec_app_inq_last_6mths`, `sec_app_mort_acc`, `sec_app_mths_since_last_major_derog`, `sec_app_num_rev_accts`, `sec_app_open_acc`, `sec_app_open_act_il`, `sec_app_revol_util`, `verification_status_joint`, `sec_app_fico_range_low`, `sec_app_fico_range_high`\n",
    "- Other: `zip_code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(927931, 74)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of rows and cols in the dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data type\n",
    "data['issue_d'] = pd.to_datetime(data['issue_d'])\n",
    "data['earliest_cr_line'] = pd.to_datetime(data['earliest_cr_line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count_Missing</th>\n",
       "      <th>Pctg_Missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mths_since_last_record</th>\n",
       "      <td>753505</td>\n",
       "      <td>0.812027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mths_since_recent_bc_dlq</th>\n",
       "      <td>696123</td>\n",
       "      <td>0.750188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mths_since_last_major_derog</th>\n",
       "      <td>659938</td>\n",
       "      <td>0.711193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mths_since_recent_revol_delinq</th>\n",
       "      <td>600515</td>\n",
       "      <td>0.647155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mths_since_last_delinq</th>\n",
       "      <td>451425</td>\n",
       "      <td>0.486486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Count_Missing  Pctg_Missing\n",
       "mths_since_last_record                 753505      0.812027\n",
       "mths_since_recent_bc_dlq               696123      0.750188\n",
       "mths_since_last_major_derog            659938      0.711193\n",
       "mths_since_recent_revol_delinq         600515      0.647155\n",
       "mths_since_last_delinq                 451425      0.486486"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values in the dataset\n",
    "def missing_values_table(data):\n",
    "    '''\n",
    "    generate a table that contains attributes and its corresponing number of missing values \n",
    "    and percentage of missing value\n",
    "    '''\n",
    "    #Number of null values by column\n",
    "    mv_df = pd.DataFrame(data.isna().sum(),columns=['Count_Missing'])\n",
    "    \n",
    "    #Portion of null values by column\n",
    "    mv_df['Pctg_Missing'] = mv_df['Count_Missing']/len(data)\n",
    "\n",
    "    #Sort by Missing_Count\n",
    "    mv_df = mv_df.sort_values('Count_Missing',ascending=False)  \n",
    "    \n",
    "    return mv_df\n",
    "\n",
    "mv_df = missing_values_table(data)\n",
    "mv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-categorize emp_length into 4 categories\n",
    "def process_emp_len(d):\n",
    "    '''\n",
    "    re_categorize the employment length in years to reduce the categories\n",
    "    '''\n",
    "    df = d.copy()\n",
    "    # 0-1 years include null, < 1 year and 1 year\n",
    "    df['emp_length'] = df['emp_length'].fillna('0-1 years')\n",
    "    df['emp_length'] = np.where(df['emp_length'].isin([np.nan,None,'< 1 year','1 year']),'0-1 years',df['emp_length'])\n",
    "\n",
    "    # 2-5 years include 2 years, 3 years, 4 years and 5 years\n",
    "    df['emp_length'] = np.where(df['emp_length'].isin(['2 years','3 years','4 years', '5 years']),'2-5 years',df['emp_length'])\n",
    "\n",
    "    # 6-9 years include 6 years, 7 years, 8 years and 9 years\n",
    "    df['emp_length'] = np.where(df['emp_length'].isin(['6 years','7 years','8 years', '9 years']),'6-9 years',df['emp_length'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# For missing values in columns that measure \"months since\": fill the nulls with maximum observed value +1\n",
    "def process_month_since_cols(d):\n",
    "    '''\n",
    "    fill the null values with the maximum observed value + 1 in 'month since' related columns, \n",
    "    the borrowers who have never had delinquencies will have the largest value for number of months \n",
    "    since delinquency.\n",
    "    '''\n",
    "    df = d.copy()\n",
    "\n",
    "    mon_since_cols = ['mo_sin_old_il_acct','mths_since_last_delinq','mths_since_last_major_derog',\n",
    "                     'mths_since_last_record','mths_since_recent_bc_dlq','mths_since_recent_inq',\n",
    "                     'mths_since_recent_revol_delinq','mo_sin_old_rev_tl_op','mths_since_recent_bc',\n",
    "                     'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl']\n",
    "\n",
    "    for i in mon_since_cols:\n",
    "        df[i].fillna(df[i].max()+1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Change grade and sub_grade to numerical rankings (the lower the number, the less risky the loan)\n",
    "def process_loan_grades(d):\n",
    "    '''\n",
    "    turn loan grade and sub_grade into ordinal mapping\n",
    "    '''\n",
    "    df = d.copy()\n",
    "\n",
    "    # turn grade into ordinal mapping\n",
    "    sorted_grades = sorted(d.grade.unique())\n",
    "    grade_dict = dict(zip(sorted_grades, range(len(sorted_grades))))\n",
    "    df['grade'] = df['grade'].map(grade_dict)\n",
    "\n",
    "    # turn sub_grade into ordinal mapping\n",
    "    sorted_subgrades = sorted(d.sub_grade.unique())\n",
    "    subgrade_dict = dict(zip(sorted_subgrades, range(len(sorted_subgrades))))\n",
    "    df['sub_grade'] = df['sub_grade'].map(subgrade_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Process the above transformation and a few more transformations\n",
    "def process_loan_cols(d):\n",
    "    df_processed = process_emp_len(d)\n",
    "    df_processed = process_month_since_cols(df_processed)\n",
    "    df_processed = process_loan_grades(df_processed)\n",
    "    \n",
    "    # add `credit_line_length` by compute the difference between the month the borrower's earliest reported credit line was opened\n",
    "    # and the month which the loan was funded. Then discard the earliest_cr_line column.\n",
    "    df_processed['credit_line_length'] = df_processed['issue_d'] - df_processed['earliest_cr_line']\n",
    "    df_processed.drop(['earliest_cr_line'], axis=1)\n",
    "\n",
    "    # process `revol_util`, `int_rate`, `credit_line_length`\n",
    "    df_processed['revol_util'] = df_processed['revol_util'].apply(lambda x: x/100)\n",
    "    df_processed['int_rate'] = df_processed['int_rate'].apply(lambda x: x/100)\n",
    "    df_processed['credit_line_length'] = df_processed['credit_line_length'].apply(lambda x: x.days)\n",
    "    \n",
    "    # generate `fully_paid` (binary variable, 0 means not defaulted, 1 means fully paid) and drop `loan_status`\n",
    "    df_processed['fully_paid'] = df_processed['loan_status'].map({'Fully Paid': 1, 'Charged Off': 0, 'Default': 0,\n",
    "                                                                  'In Grace Period': 0, 'Late (16-30 days)': 0, \n",
    "                                                                  'Late (31-120 days)': 0})\n",
    "    df_processed = df_processed.drop(columns='loan_status')\n",
    "    \n",
    "    return df_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new pre-process dataset\n",
    "data_new = process_loan_cols(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in new dataset\n",
    "mv_df = missing_values_table(data_new) # the greatest missing percentage of a feature is around 0.01\n",
    "\n",
    "# get the list of variables that have missing values\n",
    "mv_list = mv_df[mv_df['Count_Missing'] > 0].index.tolist() # all the variables that have missing values are float\n",
    "\n",
    "# impute these missing values with its median\n",
    "for var in mv_list:\n",
    "    data_new[var] = data_new[var].fillna((data_new[var].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['term',\n",
       " 'emp_length',\n",
       " 'home_ownership',\n",
       " 'verification_status',\n",
       " 'purpose',\n",
       " 'addr_state',\n",
       " 'application_type',\n",
       " 'issue_month']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the data type for each attribute\n",
    "types_data = pd.DataFrame(data_new.dtypes, columns=['Types'])\n",
    "\n",
    "# check the categorical data\n",
    "cat_var = types_data[types_data['Types'] == 'object'].index.tolist()\n",
    "cat_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for categorical variables\n",
    "data_new = pd.get_dummies(data_new, columns=cat_var, drop_first=True) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd192022b31092694831ab0308713f609d16b6fcd53b410f5770d1242282f72a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
